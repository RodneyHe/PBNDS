{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuo/anaconda3/envs/pbnds/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zhuo/anaconda3/envs/pbnds/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuo/anaconda3/envs/pbnds/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:550: UserWarning: Mtl file does not exist: /home/zhuo/remote_nfs/pbnds/models/data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "from models.decalib.deca import DECA\n",
    "from models.decalib.utils.config import cfg as deca_cfg\n",
    "from models.decalib.datasets.detectors import FAN\n",
    "\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.transform import estimate_transform, warp\n",
    "import torch\n",
    "\n",
    "# Load FLAME model and DECA model\n",
    "deca_cfg['model']['flame_model_path'] = './pretrained/generic_model.pkl'\n",
    "deca_cfg['pretrained_modelpath'] = './pretrained/deca_model.tar'\n",
    "deca_cfg['model']['flame_lmk_embedding_path'] = './pretrained/landmark_embedding.npy'\n",
    "deca_cfg['model']['use_tex'] = False\n",
    "\n",
    "deca = DECA(config=deca_cfg)\n",
    "face_detector = FAN()\n",
    "\n",
    "def bbox2point(left, right, top, bottom, type='bbox'):\n",
    "    ''' bbox from detector and landmarks are different\n",
    "    '''\n",
    "    if type=='kpt68':\n",
    "        old_size = (right - left + bottom - top)/2*1.1\n",
    "        center = torch.tensor([right - (right - left) / 2.0, bottom - (bottom - top) / 2.0 ])\n",
    "    elif type=='bbox':\n",
    "        old_size = (right - left + bottom - top)/2\n",
    "        center = torch.tensor([right - (right - left) / 2.0, bottom - (bottom - top) / 2.0  + old_size*0.12])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return old_size, center\n",
    "\n",
    "image = cv.imread('01464.png', cv.IMREAD_UNCHANGED)\n",
    "        \n",
    "if len(image.shape) == 3:\n",
    "    if image.shape[2] == 4:\n",
    "        alpha_channel = image[...,3]\n",
    "        bgr_channels = image[...,:3]\n",
    "        rgb_channels = cv.cvtColor(bgr_channels, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        # White Background Image\n",
    "        background_image = np.zeros_like(rgb_channels, dtype=np.uint8)\n",
    "        \n",
    "        # Alpha factor\n",
    "        alpha_factor = alpha_channel[:,:,np.newaxis].astype(np.float32) / 255.\n",
    "        alpha_factor = np.concatenate((alpha_factor,alpha_factor,alpha_factor), axis=2)\n",
    "\n",
    "        # Transparent Image Rendered on White Background\n",
    "        base = rgb_channels * alpha_factor\n",
    "        background = background_image * (1 - alpha_factor)\n",
    "        image = base + background\n",
    "    else:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "bbox, bbox_type = face_detector.run(image)\n",
    "\n",
    "left = bbox[0]; right=bbox[2]\n",
    "top = bbox[1]; bottom=bbox[3]\n",
    "\n",
    "old_size, center = bbox2point(left, right, top, bottom, type=bbox_type)\n",
    "size = int(old_size*1.25)\n",
    "\n",
    "src_pts = np.array([[center[0]-size/2, center[1]-size/2], [center[0] - size/2, center[1]+size/2], [center[0]+size/2, center[1]-size/2]])\n",
    "\n",
    "DST_PTS = np.array([[0, 0], [0, 223], [223, 0]])\n",
    "tform = estimate_transform('similarity', src_pts, DST_PTS)\n",
    "\n",
    "image = image / 255.\n",
    "\n",
    "dst_image = warp(image, tform.inverse, output_shape=(224, 224))\n",
    "dst_image = dst_image.transpose(2,0,1)\n",
    "with torch.no_grad():\n",
    "    codedict = deca.encode(torch.tensor(dst_image).float().cuda()[None])\n",
    "\n",
    "    tform = torch.tensor(tform.params).float()[None, ...]\n",
    "    original_image = torch.tensor(image[None]).float().cuda()\n",
    "    tform = torch.inverse(tform).transpose(1,2).cuda()\n",
    "    opdict, visdict = deca.decode(codedict, render_orig=True, original_image=original_image.permute(0,3,1,2), tform=tform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite('vis.png', deca.visualize(visdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deca.save_obj('01464.obj', opdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.ops import interpolate_face_attributes\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer.mesh.textures import TexturesUV\n",
    "\n",
    "from pytorch3d.renderer.cameras import PerspectiveCameras, look_at_view_transform\n",
    "from pytorch3d.renderer.mesh import MeshRasterizer, RasterizationSettings\n",
    "\n",
    "import torchvision.transforms.functional as tvf\n",
    "\n",
    "import utils.io as io\n",
    "\n",
    "base_path = 'dataset/test_data/'\n",
    "\n",
    "# load mesh\n",
    "verts, faces, aux = load_obj(base_path + 'OBJs/Head.obj', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sdr(image_name):\n",
    "    \n",
    "    image = cv.imread(image_name, cv.IMREAD_UNCHANGED)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    if image.dtype == 'uint8':\n",
    "        image = image / 255.\n",
    "    elif image.dtype == 'uint16':\n",
    "        image = image / 65535.\n",
    "    \n",
    "    image = cv.resize(image, (4096, 4096), interpolation=cv.INTER_NEAREST)\n",
    "    \n",
    "    return torch.from_numpy(image).float()\n",
    "\n",
    "# load textures\n",
    "basecolor = load_sdr(base_path + f'/Textures/Albedo.png').cuda()\n",
    "roughness = load_sdr(base_path + f'/Textures/Roughness_CR.png').cuda()\n",
    "specular = load_sdr(base_path + f'/Textures/Specular_CC.png').cuda()\n",
    "#normal = load_sdr(base_path + f'/Textures/Normal.png').cuda()\n",
    "#normal = torch.nn.functional.normalize(normal, dim=-1)\n",
    "\n",
    "verts_uvs = aux.verts_uvs.cuda()\n",
    "faces_uvs = faces.textures_idx.cuda()\n",
    "image = torch.cat([basecolor, roughness[...,0][..., None], specular[...,0][..., None]], dim=-1)[None]\n",
    "texture = TexturesUV(verts_uvs=[verts_uvs], faces_uvs=[faces_uvs], maps=image)\n",
    "\n",
    "mesh = Meshes(verts=[verts], faces=[faces.verts_idx], textures=texture).to('cuda')\n",
    "camera = PerspectiveCameras(in_ndc=False, image_size=[(512, 512)], device='cuda')\n",
    "rasterizer_settings = RasterizationSettings(image_size=512)\n",
    "rasterizer = MeshRasterizer(raster_settings=rasterizer_settings, cameras=camera)\n",
    "rasterizer = rasterizer.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transforms import B2P\n",
    "\n",
    "# RT = torch.tensor([[1.0, 0.0, 0.0, 0.0],\n",
    "#                    [0.0, -0.052335940301418304, -0.9986295104026794, -0.699999988079071],\n",
    "#                    [0.0, 0.9986295104026794, -0.052335940301418304, 0.800000011920929],\n",
    "#                    [0.0, 0.0, 0.0, 1.0]]).cuda()\n",
    "\n",
    "RT = torch.tensor([[0.0, 0.0, 0.0, 0.0],\n",
    "                   [0.0, -0.052335940301418304, -0.9986295104026794, -0.699999988079071],\n",
    "                   [0.0, 0.9986295104026794, -0.052335940301418304, 0.800000011920929],\n",
    "                   [0.0, 0.0, 0.0, 1.0]]).cuda()\n",
    "\n",
    "R, T, RT_4x4 = B2P(RT)\n",
    "\n",
    "#R, T = look_at_view_transform(dist=1.5, elev=-2, azim=3.1415, device='cuda') \n",
    "\n",
    "K = torch.tensor([[512., 0., 256., 0.],\n",
    "                  [0., 512., 256., 0.],\n",
    "                  [0., 0., 0., 1.],\n",
    "                  [0., 0., 1., 0.]]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    rasterizer.cameras.R = R[None]\n",
    "    rasterizer.cameras.T = T[None]\n",
    "    rasterizer.cameras.K = K[None]\n",
    "    \n",
    "    fragments = rasterizer(mesh)\n",
    "\n",
    "    textures = mesh.textures.sample_textures(fragments).squeeze(3)\n",
    "\n",
    "tvf.to_pil_image(textures[0][...,:3].permute(2,0,1)).save('test.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbnds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
