{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pbnds_weights = torch.load('../output/exp_02/weights/NeuralRenderer01.pth')\n",
    "#pbnds_w = {k: v for k, v in pbnds_weights.items() if 'unet' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pbnds_w, '../output/exp_02/weights/NeuralRenderer01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.neural_renderer import NeuralRenderer\n",
    "\n",
    "nr = NeuralRenderer()\n",
    "\n",
    "nr.load_state_dict(pbnds_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, math\n",
    "sys.path.append('..')\n",
    "os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '1'\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.neural_renderer import NeuralRenderer\n",
    "\n",
    "exp_name = 'exp_02'\n",
    "\n",
    "neural_renderer = NeuralRenderer()\n",
    "w = torch.load(f'../output/{exp_name}/weights/NeuralRenderer.pth')\n",
    "neural_renderer.load_state_dict({k: v for k, v in w.items() if 'unet' not in k})\n",
    "neural_renderer = neural_renderer.to('cuda')\n",
    "\n",
    "def get_view_pos(depth, width, height, fov):\n",
    "    fovx = math.radians(fov)\n",
    "    fovy = 2 * math.atan(math.tan(fovx / 2) / (width / height))\n",
    "    vpos = torch.zeros(height, width, 3)\n",
    "    Y = 1 - (torch.arange(height) + 0.5) / height\n",
    "    Y = Y * 2 - 1\n",
    "    X = (torch.arange(width) + 0.5) / width\n",
    "    X = X * 2 - 1\n",
    "    Y, X = torch.meshgrid(Y, X, indexing='ij')\n",
    "    vpos[..., 0] = depth * X * math.tan(fovx / 2)\n",
    "    vpos[..., 1] = depth * Y * math.tan(fovy / 2)\n",
    "    vpos[..., 2] = -depth\n",
    "    return vpos\n",
    "\n",
    "def load_sdr(image_name):\n",
    "    image = cv.imread(image_name, cv.IMREAD_UNCHANGED)\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        if image.shape[2] == 4:\n",
    "            alpha_channel = image[...,3]\n",
    "            bgr_channels = image[...,:3]\n",
    "            rgb_channels = cv.cvtColor(bgr_channels, cv.COLOR_BGR2RGB)\n",
    "            \n",
    "            # White Background Image\n",
    "            background_image = np.zeros_like(rgb_channels, dtype=np.uint8)\n",
    "            \n",
    "            # Alpha factor\n",
    "            alpha_factor = alpha_channel[:,:,np.newaxis].astype(np.float32) / 255.\n",
    "            alpha_factor = np.concatenate((alpha_factor,alpha_factor,alpha_factor), axis=2)\n",
    "\n",
    "            # Transparent Image Rendered on White Background\n",
    "            base = rgb_channels * alpha_factor\n",
    "            background = background_image * (1 - alpha_factor)\n",
    "            image = base + background\n",
    "        else:\n",
    "            image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    image = cv.resize(image, (256, 256), interpolation=cv.INTER_NEAREST)\n",
    "    \n",
    "    return torch.from_numpy(image)\n",
    "\n",
    "def load_hdr(image_name, resize=True, to_ldr=False):\n",
    "    image = cv.imread(image_name, -1)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    if resize:\n",
    "        image = cv.resize(image, (256, 256), interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "    if to_ldr:\n",
    "        image = image.clip(0, 1) ** (1 / 2.2)\n",
    "    \n",
    "    return torch.from_numpy(image)\n",
    "\n",
    "base_path = f'../dataset/ffhq256_pbr/'\n",
    "\n",
    "rgb_gt = load_sdr(base_path + f'bgremoval/06000/06112.png') / 255.\n",
    "normal_gt = load_sdr(base_path + f'texture/normal/06000/normal_06112.png')\n",
    "normal_gt = ((normal_gt / 255.) * 2 - 1).to(torch.float32)\n",
    "albedo_gt = load_sdr(base_path + f'texture/albedo/06000/albedo_06112.png') / 255.\n",
    "roughness_gt = load_sdr(base_path + f'texture/roughness/06000/roughness_06112.png') / 255.\n",
    "specular_gt = load_sdr(base_path + f'texture/specular/06000/specular_06112.png') / 255.\n",
    "depth_gt = load_hdr(base_path + f'texture/depth/06000/depth_06112.exr')[...,0]\n",
    "mask_gt = (rgb_gt != 0)[...,0]\n",
    "hdri_gt = load_hdr(base_path + f'/hdri/06000/hdri_06112.exr')\n",
    "\n",
    "view_pos_gt = get_view_pos(depth=depth_gt, width=256, height=256, fov=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_buffer = {\n",
    "    'rgb_gt': rgb_gt[mask_gt].cuda(),\n",
    "    'normal_gt': normal_gt[mask_gt].cuda(),\n",
    "    'albedo_gt': albedo_gt[mask_gt].cuda(),\n",
    "    'roughness_gt': roughness_gt[mask_gt].cuda(),\n",
    "    'specular_gt': specular_gt[mask_gt].cuda(),\n",
    "    'view_pos_gt': view_pos_gt[mask_gt].cuda(),\n",
    "    'hdri_gt': hdri_gt[None].cuda(),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    shading_rgb = neural_renderer(render_buffer, num_light_samples=128)\n",
    "\n",
    "rec_image = torch.zeros(256,256,3).cuda()\n",
    "rec_image[mask_gt] = shading_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as tvf\n",
    "\n",
    "tvf.to_pil_image(rec_image.permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w, _ = albedo_gt.shape\n",
    "\n",
    "albedo = albedo_gt.reshape(1, h*w, 1, 3)\n",
    "roughness = roughness_gt.reshape(1, h*w, 1, 1)\n",
    "specular = specular_gt.reshape(1, h*w, 1, 1)\n",
    "normal = normal_gt.reshape(1, h*w, 1, 3)\n",
    "\n",
    "# Sampling the HDRi environment map\n",
    "sampled_hdri_map, sampled_direction = neural_renderer.uniform_sampling(hdri_map=hdri_gt[None], num_samples=128)\n",
    "\n",
    "cam_pos = torch.tensor([0., 0., 0.])[None, None, :]\n",
    "\n",
    "in_dirs = sampled_direction.repeat(view_pos_gt.shape[0],1,1)                                                               # [S,N,3]\n",
    "out_dirs = (cam_pos - view_pos_gt[None].unsqueeze(1))\n",
    "out_dirs = nn.functional.normalize(out_dirs, dim=-1)                                                                    # [S,N,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdri_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dirs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_hdri_map.shape, sampled_direction.shape, view_pos_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dirs.shape, out_dirs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Repeat light for multiple pixels for sharing\n",
    "light = light.repeat_interleave(2, dim=1)\n",
    "light = light.repeat_interleave(2, dim=2)\n",
    "light = light.reshape(b,h*w,self.env_height*self.env_width,3)\n",
    "\n",
    "# Diffuse BRDF\n",
    "# diffuse_brdf = (1 - metallic) * albedo / torch.pi\n",
    "diffuse_brdf = albedo_gt\n",
    "\n",
    "# Diffuse BRDF\n",
    "half_dirs = in_dirs + out_dirs\n",
    "half_dirs = nn.functional.normalize(half_dirs, dim=-1)\n",
    "h_d_n = (half_dirs * normal).sum(dim=-1, keepdim=True).clamp(min=0)\n",
    "h_d_o = (half_dirs * out_dirs).sum(dim=-1, keepdim=True).clamp(min=0)\n",
    "n_d_i = (normal * in_dirs).sum(dim=-1, keepdim=True).clamp(min=0)\n",
    "n_d_o = (normal * out_dirs).sum(dim=-1, keepdim=True).clamp(min=0)\n",
    "\n",
    "# Fresnel term F (Schlick Approximation)\n",
    "F0 = 0.04 * (1 - metallic) + albedo * metallic\n",
    "F = F0 + (1. - F0) * ((1. - h_d_o) ** 5)\n",
    "\n",
    "# Geometry term with Smiths Approximation\n",
    "V = self.v_schlick_ggx(roughness, n_d_i) * self.v_schlick_ggx(roughness, n_d_o)\n",
    "\n",
    "# Normal distributed function (SG)\n",
    "D = self.d_sg(roughness, h_d_n).clamp(max=1)\n",
    "\n",
    "specular_brdf = D * F * V \n",
    "\n",
    "# RGB color shading\n",
    "incident_area = torch.ones_like(light) * 2 * torch.pi\n",
    "render_output = ((diffuse_brdf + specular_brdf) * light * incident_area * n_d_i).mean(dim=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbnds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
